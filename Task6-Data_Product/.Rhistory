shiny::runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
library(shiny)
library(shiny)
runApp()
runApp()
runApp()
# loading the required packages
library(shiny)
library(tidyverse)
library(stringr)
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(shiny)
library(tidyverse)
library(stringr)
runApp()
runApp()
library(stringi)
library(stringr)
library(ggplot2)
library(NLP)
library(tm)
library(RWeka)
library(RColorBrewer)
library(wordcloud)
library(SnowballC)
library(tau)
library(Matrix)
library(data.table)
library(parallel)
library(reshape2)
# reading datasets
blogs <- readLines("en_US.blogs.txt",encoding="UTF-8")
twitter <- readLines("en_US.twitter.txt",encoding="UTF-8")
news <- readLines("en_US.news.txt",encoding="UTF-8")
blogs <- iconv(blogs,"latin1","ASCII",sub="")
news <- iconv(news,"latin1","ASCII",sub="")
twitter <- iconv(twitter,"latin1","ASCII",sub="")
# length of datasets
length(blogs)
length(news)
length(twitter)
# words count of datasets
blogsWC<-stri_count_words(blogs)
sum(blogsWC)
summary(blogsWC)
newsWC <- stri_count_words(news)
sum(newsWC)
summary(newsWC)
twitterWC <- stri_count_words(twitter)
sum(twitterWC)
summary(twitterWC)
#number of characters of datasets
ncharblogs<-sum(nchar(blogs))
ncharblogs
ncharnews<-sum(nchar(news))
ncharnews
nchartwitter<-sum(nchar(twitter))
nchartwitter
library(stringi)
library(stringr)
library(ggplot2)
library(NLP)
library(tm)
library(RWeka)
library(RColorBrewer)
library(wordcloud)
library(SnowballC)
library(tau)
library(Matrix)
library(data.table)
library(parallel)
library(reshape2)
# reading datasets
blogs <- readLines("en_US.blogs.txt",encoding="UTF-8")
twitter <- readLines("en_US.twitter.txt",encoding="UTF-8")
news <- readLines("en_US.news.txt",encoding="UTF-8")
blogs <- iconv(blogs,"latin1","ASCII",sub="")
news <- iconv(news,"latin1","ASCII",sub="")
twitter <- iconv(twitter,"latin1","ASCII",sub="")
# length of datasets
length(blogs)
length(news)
length(twitter)
# words count of datasets
blogsWC<-stri_count_words(blogs)
sum(blogsWC)
summary(blogsWC)
newsWC <- stri_count_words(news)
sum(newsWC)
summary(newsWC)
twitterWC <- stri_count_words(twitter)
sum(twitterWC)
summary(twitterWC)
#number of characters of datasets
ncharblogs<-sum(nchar(blogs))
ncharblogs
ncharnews<-sum(nchar(news))
ncharnews
nchartwitter<-sum(nchar(twitter))
nchartwitter
#Take a sample of 500 from each dataset
blogsSample<-sample(blogs,500,replace=FALSE)
newsSample<-sample(news,500,replace=FALSE)
twitterSample<-sample(twitter,500,replace=FALSE)
#combine samples
sample<-c(blogsSample,newsSample,twitterSample)
#load sample data as corpus
corpus<-VCorpus(VectorSource(sample))
corpus
#Text transformation
toSpace<-content_transformer(function(x,pattern)gsub(pattern,"",x))
corpus<-tm_map(corpus,toSpace,"/")
corpus<-tm_map(corpus,toSpace,"@")
corpus<-tm_map(corpus,toSpace,"\\|")
#Cleaning the text
corpus<-tm_map(corpus,content_transformer(tolower))
corpus<-tm_map(corpus,removeNumbers)
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,stripWhitespace)
corpus<-tm_map(corpus,stemDocument)
tdm <- TermDocumentMatrix(corpus)
tdm
inspect(tdm)
m <- as.matrix(tdm)
m1 <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word=names(m1),freq=m1)
head(d,20)
set.seed(123)
wordcloud(words=d$word,d$freq,min.freq=10,max.words=200,random.order=FALSE,scale=c(8,0.5),colors=brewer.pal(8,"Dark2"))
barplot(d[1:20,]$freq,las=2,names.arg=d[1:20,]$word,col="dodgerblue2",main="Most Frequent Words",ylab="Word Frequencies")
# creating unigram
UnigramTokenizer<-function(x)NGramTokenizer(x,Weka_control(min=1,max=1))
unigram<-TermDocumentMatrix(corpus,control=list(tokenize=UnigramTokenizer))
# extracting frequencies of the unigram
unifreq<-sort(rowSums(as.matrix(unigram)),decreasing=TRUE)
unifreqDF<-data.frame(word1=names(unifreq),freq=unifreq)
head(unifreqDF,10)
# creating Bigram
BigramTokenizer<-function(x)NGramTokenizer(x,Weka_control(min=2,max=2))
bigram<-TermDocumentMatrix(corpus,control=list(tokenize=BigramTokenizer))
# extracting frequencies of the bigram
bifreq<-sort(rowSums(as.matrix(bigram)),decreasing=TRUE)
bifreqDF<-data.frame(word2=names(bifreq),freq=bifreq)
head(bifreqDF,10)
# creating Trigram
TrigramTokenizer<-function(x)NGramTokenizer(x,Weka_control(min=3,max=3))
trigram<-TermDocumentMatrix(corpus,control=list(tokenize=TrigramTokenizer))
# extracting frequencies of the trigram
trifreq<-sort(rowSums(as.matrix(trigram)),decreasing=TRUE)
trifreqDF<-data.frame(word3=names(trifreq),freq=trifreq)
head(trifreqDF,10)
set.seed(123)
wordcloud(words=unifreqDF$word1,unifreqDF$freq,min.freq=10,max.words=200,random.order=FALSE,scale=c(8,0.5),colors=brewer.pal(5,"Dark2"))
wordcloud(words=bifreqDF$word2,bifreqDF$freq,min.freq=5,max.words=100,random.order=FALSE,scale=c(3,0.5),colors=brewer.pal(5,"Dark2"))
wordcloud(words=trifreqDF$word3,trifreqDF$freq,min.freq=5,max.words=100,random.order=FALSE,scale=c(2,0.1),colors=brewer.pal(5,"Dark2"))
# barplot of unigrams
names(unifreq)=c("the","and","that","for","you","was","with","have","this","but")
barplot(unifreqDF[1:10,]$freq,las=2,names.arg=unifreqDF[1:10,]$word1,col="dodgerblue2",cex.names=0.8,ylab="Frequency",main="Most Frequent Unigrams")
# barplot of bigrams
names(bifreq)=c("of the","in the","to the","for the","on the","to be","at the","and the","in a","with the")
barplot(bifreqDF[1:10,]$freq,las=2,names.arg=bifreqDF[1:10,]$word2,col="dodgerblue2",cex.names=0.8,ylab="Frequency",main="Most Frequent Bigrams")
# barplot of trigrams
names(trifreq)=c("one of the","a lot of","i want to","be abl to","part of the","thank for the","if you have","it was a","out of the","to be a")
barplot(trifreqDF[1:10,]$freq,las=2,names.arg=trifreqDF[1:10,]$word3,col="dodgerblue2",cex.names=0.7,ylab="Frequency",main="Most Frequent Trigrams")
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
